# ğŸ”‘ Fuzzy Hashing Tool

A simple Python-based **fuzzy hashing tool** that uses [ssdeep](https://ssdeep-project.github.io/ssdeep/) to detect **similarities between files**.  
Unlike normal hashing (MD5/SHA256) that only detects identical files, fuzzy hashing can tell if two files are **partially similar**, which is useful in **malware analysis, forensics, and duplicate detection**.

---

## âš™ï¸ Features
- Generate **fuzzy hashes** for files
- Compare **two files** and output similarity score (0â€“100%)
- Scan a **directory** and print fuzzy hashes for all files
- Works on **Linux (Kali, Ubuntu, etc.)** and **Windows**

---

## ğŸ“¦ Dependencies

### Linux (Debian/Kali/Ubuntu)
```bash
sudo apt update
sudo apt install ssdeep python3-ssdeep python3-pip -y
pip install ssdeep


Windows

Install Python 3
.

Open Command Prompt (cmd) or PowerShell and run:
pip install ssdeep




ğŸš€ Usage

Clone or copy the script into a file named fuzzy.py.

1. Compare two files
python3 fuzzy.py -c file1.txt file2.txt


âœ… Outputs fuzzy hashes of both files and a similarity score.

2. Scan all files in a directory
python3 fuzzy.py -d ./samples


âœ… Prints fuzzy hashes for every file in the folder.



ğŸ› ï¸ Example
echo "The quick brown fox jumps over the lazy dog." > file1.txt
echo "The quick brown fox jumps over the smart dog." > file2.txt

python3 fuzzy.py -c file1.txt file2.txt


Output:

ğŸ”¹ File 1: file1.txt
   Hash: 3:kU4h8:foxdog
ğŸ”¹ File 2: file2.txt
   Hash: 3:kU4h9:foxdog

âœ… Similarity Score: 86%


ğŸ” How It Works

Input â†’ User provides files or directory.

Chunking â†’ Files are split into blocks instead of hashing whole file.

Rolling Hash â†’ Each chunk is hashed with a rolling algorithm.

Signature Generation â†’ The sequence of chunk-hashes forms a fuzzy hash signature.

Comparison â†’ Fuzzy hashes are compared using an edit distance algorithm.

Output â†’ Tool prints fuzzy hashes and a similarity score (0â€“100%).

ğŸ¯ Use Cases

Malware variant detection

Digital forensics

Detecting modified or tampered files

Finding near-duplicate documents
